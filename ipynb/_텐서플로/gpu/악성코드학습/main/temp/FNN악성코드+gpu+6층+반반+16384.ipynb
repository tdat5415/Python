{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\___Anaconda\\envs\\forTFgpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.0001 #0.001\n",
    "training_epochs = 120 #15\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#인코딩중요\n",
    "#라벨만들기\n",
    "with open('./trainSet.csv', 'r', encoding='UTF-8-sig') as raw:\n",
    "    lines = raw.readlines()\n",
    "data = list(csv.reader(lines))\n",
    "data = sorted(data, key=lambda x: x[0])\n",
    "\n",
    "#이름빼기\n",
    "_Y = [y[1] for y in data]\n",
    "\n",
    "_Y = np.array(_Y)\n",
    "_Y = _Y.astype(float) # shape(-1)\n",
    "_Y = [[1-y, y] for y in _Y] # shape(-1, 2)\n",
    "\n",
    "\n",
    "#데이터만들기\n",
    "with open('./features16384.csv', 'r', encoding='UTF-8-sig') as raw:\n",
    "    lines = raw.readlines()\n",
    "data = list(csv.reader(lines))\n",
    "data = sorted(data, key=lambda x: x[0])\n",
    "\n",
    "#이름빼기\n",
    "_X = [x[1:] for x in data]\n",
    "\n",
    "_X = np.array(_X)\n",
    "_X = _X.astype(float) # shape(-1, 1024)\n",
    "\n",
    "\n",
    "all_set = tuple(zip(_X, _Y)) # shape(10000, 2, ~)\n",
    "M_all_set = [all_set[i] for i in range(len(all_set)) if all_set[i][1][1] == 1] # 7000\n",
    "N_all_set = [all_set[i] for i in range(len(all_set)) if all_set[i][1][0] == 1] # 3000\n",
    "np.random.shuffle(M_all_set)\n",
    "np.random.shuffle(N_all_set)\n",
    "\n",
    "# 악성,정상 개수 맞추기\n",
    "if len(M_all_set) > len(N_all_set):\n",
    "    M_all_set = M_all_set[:len(N_all_set)]\n",
    "else:\n",
    "    N_all_set = N_all_set[:len(M_all_set)]\n",
    "\n",
    "all_set = M_all_set + N_all_set\n",
    "np.random.shuffle(all_set) # 6000\n",
    "\n",
    "train_set = all_set[len(all_set)//5:] # 4800\n",
    "test_set = all_set[:len(all_set)//5] # 1200\n",
    "\n",
    "train_X = np.array([X for X,Y in train_set])\n",
    "train_Y = np.array([Y for X,Y in train_set])\n",
    "test_X = np.array([X for X,Y in test_set])\n",
    "test_Y = np.array([Y for X,Y in test_set])\n",
    "\n",
    "\n",
    "M_test_X = [X for X,Y in test_set if Y[1] == 1] # label의 악성이면\n",
    "M_test_Y = [Y for X,Y in test_set if Y[1] == 1] # label의 악성이면\n",
    "\n",
    "N_test_X = [X for X,Y in test_set if Y[0] == 1] # label의 정상이면\n",
    "N_test_Y = [Y for X,Y in test_set if Y[0] == 1] # label의 정상이면\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-3646218243ef>:12: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-3646218243ef>:50: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    X = tf.placeholder(tf.float32, [None, 16384])\n",
    "    Y = tf.placeholder(tf.float32, [None, 2])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    #X = tf.nn.dropout(X, keep_prob=keep_prob)\n",
    "\n",
    "    # Layer1\n",
    "    W1 = tf.get_variable('%f'%(np.random.random()), shape=[16384,2000], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([2000]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "    # Layer2\n",
    "    W2 = tf.get_variable('%f'%(np.random.random()), shape=[2000,1000], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([1000]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "    # Layer3\n",
    "    W3 = tf.get_variable('%f'%(np.random.random()), shape=[1000,500], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([500]))\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "    # Layer4\n",
    "    W4 = tf.get_variable('%f'%(np.random.random()), shape=[500,250], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([250]))\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "    \n",
    "    # Layer5\n",
    "    W5 = tf.get_variable('%f'%(np.random.random()), shape=[250,100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([100]))\n",
    "    L5 = tf.nn.relu(tf.matmul(L4, W5) + b5)\n",
    "    L5 = tf.nn.dropout(L5, keep_prob=keep_prob)\n",
    "    \n",
    "    # Layer6\n",
    "    #W7 = tf.get_variable('%f'%(np.random.random()), shape=[250,100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #b7 = tf.Variable(tf.random_normal([100]))\n",
    "    #L7 = tf.nn.relu(tf.matmul(L6, W7) + b7)\n",
    "    #L7 = tf.nn.dropout(L7, keep_prob=keep_prob)\n",
    "    \n",
    "    # Layer7\n",
    "    W8 = tf.get_variable('%f'%(np.random.random()), shape=[100,2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b8 = tf.Variable(tf.random_normal([2]))\n",
    "    hypothesis = tf.matmul(L5, W8) + b8\n",
    "\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0001 cost =  0.896880013\n",
      "Epoch :  0002 cost =  0.726698865\n",
      "Epoch :  0003 cost =  0.647498799\n",
      "Epoch :  0004 cost =  0.563102323\n",
      "Epoch :  0005 cost =  0.497752474\n",
      "Epoch :  0006 cost =  0.448730347\n",
      "Epoch :  0007 cost =  0.419927279\n",
      "Epoch :  0008 cost =  0.346005846\n",
      "Epoch :  0009 cost =  0.336164968\n",
      "Epoch :  0010 cost =  0.298675640\n",
      "Epoch :  0011 cost =  0.272020503\n",
      "Epoch :  0012 cost =  0.295326917\n",
      "Epoch :  0013 cost =  0.245783939\n",
      "Epoch :  0014 cost =  0.245997365\n",
      "Epoch :  0015 cost =  0.213649319\n",
      "Epoch :  0016 cost =  0.203172852\n",
      "Epoch :  0017 cost =  0.245982094\n",
      "Epoch :  0018 cost =  0.199813040\n",
      "Epoch :  0019 cost =  0.182088919\n",
      "Epoch :  0020 cost =  0.167760689\n",
      "Epoch :  0021 cost =  0.182524446\n",
      "Epoch :  0022 cost =  0.187516267\n",
      "Epoch :  0023 cost =  0.157641626\n",
      "Epoch :  0024 cost =  0.169205213\n",
      "Epoch :  0025 cost =  0.182187143\n",
      "Epoch :  0026 cost =  0.165305433\n",
      "Epoch :  0027 cost =  0.148382151\n",
      "Epoch :  0028 cost =  0.137338886\n",
      "Epoch :  0029 cost =  0.147294872\n",
      "Epoch :  0030 cost =  0.151129966\n",
      "Epoch :  0031 cost =  0.174824399\n",
      "Epoch :  0032 cost =  0.119177745\n",
      "Epoch :  0033 cost =  0.125408589\n",
      "Epoch :  0034 cost =  0.151944847\n",
      "Epoch :  0035 cost =  0.132520484\n",
      "Epoch :  0036 cost =  0.129342895\n",
      "Epoch :  0037 cost =  0.144941689\n",
      "Epoch :  0038 cost =  0.168504437\n",
      "Epoch :  0039 cost =  0.126289795\n",
      "Epoch :  0040 cost =  0.108197181\n",
      "Epoch :  0041 cost =  0.093889870\n",
      "Epoch :  0042 cost =  0.131060382\n",
      "Epoch :  0043 cost =  0.106556295\n",
      "Epoch :  0044 cost =  0.097364046\n",
      "Epoch :  0045 cost =  0.106122275\n",
      "Epoch :  0046 cost =  0.111289995\n",
      "Epoch :  0047 cost =  0.099131364\n",
      "Epoch :  0048 cost =  0.112371873\n",
      "Epoch :  0049 cost =  0.087328434\n",
      "Epoch :  0050 cost =  0.090784532\n",
      "Epoch :  0051 cost =  0.080379508\n",
      "Epoch :  0052 cost =  0.081425705\n",
      "Epoch :  0053 cost =  0.097908980\n",
      "Epoch :  0054 cost =  0.096697476\n",
      "Epoch :  0055 cost =  0.082738127\n",
      "Epoch :  0056 cost =  0.075883659\n",
      "Epoch :  0057 cost =  0.119043427\n",
      "Epoch :  0058 cost =  0.099670742\n",
      "Epoch :  0059 cost =  0.097603880\n",
      "Epoch :  0060 cost =  0.088052257\n",
      "Epoch :  0061 cost =  0.079494222\n",
      "Epoch :  0062 cost =  0.068327476\n",
      "Epoch :  0063 cost =  0.088259034\n",
      "Epoch :  0064 cost =  0.076607882\n",
      "Epoch :  0065 cost =  0.077877554\n",
      "Epoch :  0066 cost =  0.131964137\n",
      "Epoch :  0067 cost =  0.085635913\n",
      "Epoch :  0068 cost =  0.076582805\n",
      "Epoch :  0069 cost =  0.069584182\n",
      "Epoch :  0070 cost =  0.096075735\n",
      "Epoch :  0071 cost =  0.097770520\n",
      "Epoch :  0072 cost =  0.074898529\n",
      "Epoch :  0073 cost =  0.082466718\n",
      "Epoch :  0074 cost =  0.107776550\n",
      "Epoch :  0075 cost =  0.133942338\n",
      "Epoch :  0076 cost =  0.163555609\n",
      "Epoch :  0077 cost =  0.122845355\n",
      "Epoch :  0078 cost =  0.139683872\n",
      "Epoch :  0079 cost =  0.092099573\n",
      "Epoch :  0080 cost =  0.067581290\n",
      "Epoch :  0081 cost =  0.056811313\n",
      "Epoch :  0082 cost =  0.061073999\n",
      "Epoch :  0083 cost =  0.069148240\n",
      "Epoch :  0084 cost =  0.077078944\n",
      "Epoch :  0085 cost =  0.066825514\n",
      "Epoch :  0086 cost =  0.072475856\n",
      "Epoch :  0087 cost =  0.052848018\n",
      "Epoch :  0088 cost =  0.065592592\n",
      "Epoch :  0089 cost =  0.068410394\n",
      "Epoch :  0090 cost =  0.054556057\n",
      "Epoch :  0091 cost =  0.072435126\n",
      "Epoch :  0092 cost =  0.057005396\n",
      "Epoch :  0093 cost =  0.054802966\n",
      "Epoch :  0094 cost =  0.074670737\n",
      "Epoch :  0095 cost =  0.067072258\n",
      "Epoch :  0096 cost =  0.050557820\n",
      "Epoch :  0097 cost =  0.046679057\n",
      "Epoch :  0098 cost =  0.067132335\n",
      "Epoch :  0099 cost =  0.069251706\n",
      "Epoch :  0100 cost =  0.052778666\n",
      "Epoch :  0101 cost =  0.060973741\n",
      "Epoch :  0102 cost =  0.051936534\n",
      "Epoch :  0103 cost =  0.077295476\n",
      "Epoch :  0104 cost =  0.060904712\n",
      "Epoch :  0105 cost =  0.059891335\n",
      "Epoch :  0106 cost =  0.047876502\n",
      "Epoch :  0107 cost =  0.044593208\n",
      "Epoch :  0108 cost =  0.063160195\n",
      "Epoch :  0109 cost =  0.047517228\n",
      "Epoch :  0110 cost =  0.051419391\n",
      "Epoch :  0111 cost =  0.048431547\n",
      "Epoch :  0112 cost =  0.050568756\n",
      "Epoch :  0113 cost =  0.052340770\n",
      "Epoch :  0114 cost =  0.057308416\n",
      "Epoch :  0115 cost =  0.050236285\n",
      "Epoch :  0116 cost =  0.071425829\n",
      "Epoch :  0117 cost =  0.065595264\n",
      "Epoch :  0118 cost =  0.047532765\n",
      "Epoch :  0119 cost =  0.056206269\n",
      "Epoch :  0120 cost =  0.057784294\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(len(train_X) / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = train_X[i*batch_size:(i+1)*batch_size], train_Y[i*batch_size:(i+1)*batch_size]\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        #feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        \n",
    "        avg_cost += c / total_batch #뭉치갯수로 나누기\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "    print('Epoch : ', '%04d' % (epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9450\n",
      "정탐율 : 0.9460\n",
      "오탐율 : 0.9440\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "#feed_dict = {X: test_X, Y: test_Y, keep_prob: 1}\n",
    "#print(\"Accuracy : \", accuracy.eval(session=sess, feed_dict=feed_dict))\n",
    "\n",
    "#feed_dict = {X: M_test_X, Y: M_test_Y, keep_prob: 1}\n",
    "#print(\"정탐율 : \", accuracy.eval(session=sess, feed_dict=feed_dict))\n",
    "\n",
    "#feed_dict = {X: N_test_X, Y: N_test_Y, keep_prob: 1}\n",
    "#print(\"오탐율 : \", 1 - accuracy.eval(session=sess, feed_dict=feed_dict))\n",
    "\n",
    "total_batch = math.ceil(len(test_X) / batch_size)\n",
    "total_acc = 0\n",
    "for i in range(total_batch):\n",
    "    batch_xs, batch_ys = test_X[i*batch_size:(i+1)*batch_size], test_Y[i*batch_size:(i+1)*batch_size]\n",
    "    feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 1}\n",
    "    one_acc = accuracy.eval(session=sess, feed_dict=feed_dict)\n",
    "    total_acc += one_acc * len(batch_ys)\n",
    "print(\"Accuracy : %0.4f\"%(total_acc/len(test_Y)))\n",
    "\n",
    "\n",
    "total_batch = math.ceil(len(M_test_X) / batch_size)\n",
    "total_acc = 0\n",
    "for i in range(total_batch):\n",
    "    batch_xs, batch_ys = M_test_X[i*batch_size:(i+1)*batch_size], M_test_Y[i*batch_size:(i+1)*batch_size]\n",
    "    feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 1}\n",
    "    one_acc = accuracy.eval(session=sess, feed_dict=feed_dict)\n",
    "    total_acc += one_acc * len(batch_ys)\n",
    "print(\"정탐율 : %0.4f\"%(total_acc/len(M_test_Y)))\n",
    "\n",
    "\n",
    "total_batch = math.ceil(len(N_test_X) / batch_size)\n",
    "total_acc = 0\n",
    "for i in range(total_batch):\n",
    "    batch_xs, batch_ys = N_test_X[i*batch_size:(i+1)*batch_size], N_test_Y[i*batch_size:(i+1)*batch_size]\n",
    "    feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 1}\n",
    "    one_acc = accuracy.eval(session=sess, feed_dict=feed_dict)\n",
    "    total_acc += one_acc * len(batch_ys)\n",
    "print(\"오탐율 : %0.4f\"%(total_acc/len(N_test_Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
